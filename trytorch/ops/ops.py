from typing import Optional, Union
from ..autograd import Tensor, TensorOp
from ..array_api import array_api, NDArray
from ..array_device import get_device_by_data
import numpy
import cupy


'''
    æ‰€æœ‰ç®—å­ç»§æ‰¿TensorOp,å®ç°å‰å‘è®¡ç®—å’Œæ¢¯åº¦è®¡ç®—(åå‘ä¼ æ’­)
    å‰å‘è¿‡ç¨‹è°ƒç”¨é“¾:
    ç”ŸæˆTensorOpå­ç±»çš„å®ä¾‹ -> 
    è°ƒç”¨__call__å‡½æ•° -> 
    è°ƒç”¨make_from_op -> 
    è°ƒç”¨realize_cached_data -> 
    è°ƒç”¨op.computeå®Œæˆè®¡ç®—

    ç®—å­ä¹¦å†™è§„åˆ™:
        å‰å‘è®¡ç®—,ä½¿ç”¨array_apiæ“ä½œNDArray
        åŒ…è£…å‰å‘è®¡ç®—(Tensorè®¡ç®—)ä¸ºç”¨æˆ·æ¥å£func()
        åå‘ä¼ æ’­,ä½¿ç”¨func()è®¡ç®—æ¢¯åº¦
'''

# Tensorç›¸åŠ 
class EWiseAdd(TensorOp):
    # å®ç°Valueç±»çš„compute
    def compute(self, a: NDArray, b: NDArray):
        return a + b
    
    '''
    y = a + b   
    âˆ‚y/âˆ‚a = 1  vÂ¯{a-y} = grad_y * 1 = grad_y
    âˆ‚y/âˆ‚b = 1  vÂ¯{b-y} = grad_y * 1 = grad_y
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        return out_grad, out_grad
    

'''
è°ƒç”¨é“¾
EWiseAdd().__call__().make_from_op().realize_cached_data().compute()
'''
# ç›´è§‚çš„ç”¨æˆ·æ¥å£
def add(a: Tensor, b: Tensor):
    return EWiseAdd()(a, b)
    


# Tensor åŠ  æ ‡é‡    ä¸ä¸Šé¢ç›¸æ¯”,æ¢¯åº¦åä¼ ä¸åŒ!
class AddScalar(TensorOp):
    def __init__(self, scalar):
        self.scalar = scalar

    def compute(self, a: NDArray):
        return a + self.scalar
    
    '''
    y = a + C
    âˆ‚y/âˆ‚a = 1    vÂ¯{a-y} = grad_y * 1 = grad_y
    Cæ˜¯éå¼ é‡,ä¸éœ€ä¼ æ’­æ¢¯åº¦
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        return (out_grad,)
    

def add_scalar(a, scalar):
    return AddScalar(scalar)(a)



# å‡æ³•ç®—å­ç›´æ¥ä½¿ç”¨add(a, -b)
# åªéœ€æä¾›è´Ÿå·ç®—å­
class Negate(TensorOp):
    def compute(self, a: NDArray):
        return -a
    '''
    y = -a
    âˆ‚y/âˆ‚a = -1  vÂ¯{a-y} = -grad_y 
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        return negate(out_grad)


def negate(a):
    return Negate()(a)



# Tensor ç›¸ä¹˜
class EWiseMul(TensorOp):
    def compute(self, a: NDArray, b: NDArray):
        return a * b
    
    '''
    y = a * b
    âˆ‚y/âˆ‚a = b  vÂ¯{a-y} = grad_y * b 
    âˆ‚y/âˆ‚b = a  vÂ¯{b-y} = grad_y * a 
    æ‰€ä»¥è¿˜éœ€è¦çŸ¥é“a, bçš„èŠ‚ç‚¹å€¼
    å­˜å‚¨åœ¨yçš„input: List[Tensor] ç»“æ„ä¸­
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a, b = node.inputs    
        return b * out_grad, a * out_grad
    

def multiply(a, b):
    return EWiseMul()(a, b)



# Tensor ä¸ æ ‡é‡ç›¸ä¹˜    åŒç†ç”±äºæ¢¯åº¦åä¼ ä¸åŒæ‰€ä»¥éœ€é¢å¤–æ„å»ºç®—å­
class MulScalar(TensorOp):
    def __init__(self, scalar):
        self.scalar = scalar

    def compute(self, a: NDArray):
        return a * self.scalar

    '''
    y = a * C
    âˆ‚y/âˆ‚a = C   vÂ¯{a-y} = grad_y * C
    Cæ ‡é‡ä¸åä¼ æ¢¯åº¦
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        return (out_grad * self.scalar,)  


def mul_scalar(a, scalar):
    return MulScalar(scalar)(a)



# Tensor çš„ scalar æ¬¡å¹‚
class PowerScalar(TensorOp):
    def __init__(self, scalar: int):
        self.scalar = scalar

    # array_apiä¾æ®aç±»å‹é€‰æ‹©numpy OR cupy
    def compute(self, a: NDArray) -> NDArray:
        return array_api.power(a, self.scalar)
    
    '''
    y = a ** C
    âˆ‚y/âˆ‚a = Ca ** (C - 1)  vÂ¯{a-y} = grad_y * Ca ** (C - 1) 
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        return out_grad * self.scalar * power_scalar(a, self.scalar - 1)
    

def power_scalar(a, scalar):
    return PowerScalar(scalar)(a)



# Tensoré—´é™¤æ³•
class EWiseDiv(TensorOp):
    def compute(self, a: NDArray, b: NDArray):
        return a / b
    
    '''
    y = a / b
    âˆ‚y/âˆ‚a = 1 / b           vÂ¯{a-y} = grad_y * 1 / b 
    âˆ‚y/âˆ‚b = - a / b**2      vÂ¯{b-y} = grad_y * (- a / b**2 ) 
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a, b = node.inputs
        return out_grad * (power_scalar(b, -1)), out_grad * (divide(negate(a), power_scalar(b, 2)))


def divide(a, b):
    return EWiseDiv()(a, b)



# Tensor é™¤ä»¥ æ ‡é‡
class DivScalar(TensorOp):
    def __init__(self, scalar):
        self.scalar = scalar

    def compute(self, a: NDArray):
        return a / self.scalar
    
    '''
    y = a / C
    âˆ‚y/âˆ‚a = 1 / C           vÂ¯{a-y} = grad_y * 1 / C 
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        return (out_grad / self.scalar,)


def divide_scalar(a, scalar):
    return DivScalar(scalar)(a)



class Transpose(TensorOp):
    def __init__(self, axes: Optional[tuple] = None):
        self.axes = axes

    # é€šè¿‡äº¤æ¢çŸ©é˜µshapeå‚æ•°"å®ç°"è½¬ç§©
    def compute(self, a: NDArray):
        # æœªè½¬ç½®æ—¶ç»´åº¦ [1,2,3,4]
        self.axis_l = list(range(a.ndim))
        # ä¸æä¾›å‚æ•°é»˜è®¤è½¬åä¸¤ç»´
        if self.axes is None:
            self.axis_l[-1], self.axis_l[-2] = self.axis_l[-2], self.axis_l[-1]
        else:
            self.axis_l[self.axes[0]], self.axis_l[self.axes[1]] = self.axes[1], self.axes[0]
        
        return array_api.transpose(a, self.axis_l)
    
    '''
    è®¸å¤šçº¿æ€§æ“ä½œçš„æ¢¯åº¦ä¼ æ’­å°±æ˜¯è¯¥æ“ä½œçš„é€†æ“ä½œã€‚
    å¯¹ä¸Šæ¸¸æ¢¯åº¦è¿›è¡Œç›¸åŒè½¬ç½®æ“ä½œå³å®ç°ä¼ æ’­
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        return transpose(out_grad, self.axes)


def transpose(a, axes=None):
    return Transpose(axes)(a)



class Reshape(TensorOp):
    def __init__(self, shape):
        self.shape = shape

    def compute(self, a: NDArray):
        return array_api.reshape(a, self.shape)

    '''
    è®¸å¤šçº¿æ€§æ“ä½œçš„æ¢¯åº¦ä¼ æ’­å°±æ˜¯è¯¥æ“ä½œçš„é€†æ“ä½œã€‚
    å¯¹ä¸Šæ¸¸æ¢¯åº¦è¿›è¡Œç›¸åŒReshapeæ“ä½œå³å®ç°ä¼ æ’­
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        return reshape(out_grad, a.shape)


def reshape(a, shape):
    return Reshape(shape)(a)



class BroadcastTo(TensorOp):
    def __init__(self, shape):
        self.shape = shape

    def compute(self, a: NDArray):
        return array_api.broadcast_to(a, self.shape)

    '''
    è®¸å¤šçº¿æ€§æ“ä½œçš„æ¢¯åº¦ä¼ æ’­å°±æ˜¯è¯¥æ“ä½œçš„é€†æ“ä½œã€‚
    å¯¹ä¸Šæ¸¸æ¢¯åº¦åœ¨å¹¿æ’­å¢åŠ çš„ç»´åº¦ä¸Šè¿›è¡Œæ±‚å’Œï¼Œç„¶åé‡å¡‘å›åŸå§‹è¾“å…¥å½¢çŠ¶ã€‚
    e.g. [1, 2] -> [1, 2]   [g1, g2] -> [g1 + g3, g2 + g4]
                   [1, 2]   [g3, g4]
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        in_shape = node.inputs[0].shape
        out_shape = out_grad.shape

        # å¯¹é½ç»´åº¦,å…ˆè·å–å…ƒç»„é•¿åº¦å·®åˆ«,å†æ ¹æ®å·®åˆ«å‘å‰è¡¥1
        diff_len = len(out_shape) - len(in_shape)
        # æ³¨æ„å…ƒç»„é—´è®¡ç®—ç›´æ¥æ˜¯åŠ å½¢çŠ¶(æ‹¼æ¥)è€Œéæ•°å€¼
        in_shape_aligned = (1,) * diff_len + in_shape

        # å¯»æ‰¾æ‹¼æ¥çš„ç»´åº¦,å¯¹è¯¥ç»´åº¦æ±‚å’Œ
        axes = []
        for i, (in_dim, out_dim) in enumerate(zip(in_shape_aligned, out_shape)):
            if in_dim == 1 and out_dim > 1:
                axes.append(i)

        grad = out_grad.sum(axes=tuple(axes))
        # sum å¯èƒ½ä¼šèˆå»ç»´åº¦, æˆ‘ä»¬æ‰‹åŠ¨æ¢å¤
        return grad.reshape(in_shape)


def broadcast_to(a, shape):
    return BroadcastTo(shape)(a)



class Summation(TensorOp):
    def __init__(self, axes: Optional[Union[tuple, int]] = None):
        self.axes = axes
        # ç¡®ä¿ä¸€å®šä¸ºtuple
        if isinstance(self.axes, int):
            self.axes = tuple([self.axes])

    def compute(self, a: NDArray):
        # numpy é»˜è®¤ keepdim = False
        return array_api.sum(a, self.axes)
    
    '''
    è®¸å¤šçº¿æ€§æ“ä½œçš„æ¢¯åº¦ä¼ æ’­å°±æ˜¯è¯¥æ“ä½œçš„é€†æ“ä½œã€‚
    åœ¨åå‘ä¼ æ’­æ—¶,ä¸Šæ¸¸æ¢¯åº¦ä¼šè¢«å‡åŒ€åˆ†é…åˆ°æ‰€æœ‰è¢«æ±‚å’Œçš„å…ƒç´ ã€‚
    e.g. [1, 2, 3] => [6,    [g1, => [g1  g1  g1]
         [4, 5, 6]     15]    g2]    [g2  g2  g2]
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        input_shape = node.inputs[0].shape
        final_shape = list(input_shape)

        # è¿˜åŸè¢«å‹ç¼©çš„ç»´åº¦
        # e.g.    sum[(2,3) axes = 1] -> (2) => (2, 1)  
        if self.axes:
            for dim in self.axes:
                final_shape[dim] = 1
        # numpyä¸­axes = None åˆ™æ±‚å…¨éƒ¨å’Œå¹¶è¿”å›æ ‡é‡
        # e.g.   sum[(2,3) axes = None] -> _ => (1, 1)  
        else:
            final_shape = [1 for _ in range(len(final_shape))]

        # å®Œæˆå½¢çŠ¶è½¬æ¢
        out_grad = reshape(out_grad, final_shape)

        # å¹¿æ’­grad
        # e.g. [g1, g2] * [1  1  1] = [g1 g1 g1]
        #                 [1  1  1]   [g2 g2 g2]
        return out_grad * array_api.ones(input_shape,dtype="float32",device=out_grad.device)
    

def summation(a, axes=None):
    return Summation(axes)(a)



# çŸ©é˜µä¹˜æ³•
class MatMul(TensorOp):
    def compute(self, a: NDArray, b: NDArray):
        return array_api.matmul(a, b)
    
    '''
    y = A @ B   
    A: (m, k)  
    B: (k, n) 
    y, grad  (m, n)
    âˆ‚y/âˆ‚A = B^T          : (n, k)          
    âˆ‚y/âˆ‚B = A^T          : (k, m)      
    gradA = grad @ B^T   : (m, k)
    gradB = A^T  @ grad  : (k, n)
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        A, B = node.inputs
        gradA = matmul(out_grad, transpose(B))
        gradB = matmul(transpose(A), out_grad)

        # å¤„ç†å¹¿æ’­æœºåˆ¶å¸¦æ¥çš„ç»´åº¦ä¸åŒ¹é…é—®é¢˜ã€‚
        if gradA.shape != A.shape:
            gradA = summation(gradA, tuple(range(len(gradA.shape) - len(A.shape))))
        if gradB.shape != B.shape:
            gradB = summation(gradB, tuple(range(len(gradB.shape) - len(B.shape))))

        return gradA, gradB


def matmul(a, b):
    return MatMul()(a, b)



# logå‡½æ•°
class Log(TensorOp):
    def compute(self, a: NDArray):
        return array_api.log(a)
    
    '''
    y = ln(a)  
    âˆ‚y/âˆ‚a = 1 / a   
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        return out_grad * power_scalar(a, -1)


def log(a):
    return Log()(a)



# sinå‡½æ•°
class Sin(TensorOp):
    def compute(self, a: NDArray):
        return array_api.sin(a)
    '''
    y = sin(a)  
    âˆ‚y/âˆ‚a = cos(a)   
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        '''
            âŒ ä¸¥é‡é”™è¯¯: return out_grad * array_api.cos(a)
            array_apiæ˜¯æ“ä½œNDArrayçš„,ç”¨äºå®ç°å‰å‘è®¡ç®—ä¸­æ•°å€¼çš„çœŸæ­£è®¡ç®—
            è¿›è¡ŒTensorè¿ç®—,å¿…é¡»ä½¿ç”¨opsä¸­å°è£…å¥½çš„å‡½æ•°
        '''
        return out_grad * cos(a)


def sin(a):
    return Sin()(a)



class Cos(TensorOp):
    def compute(self, a: NDArray):
        return array_api.cos(a)

    '''
    y = cos(a)  
    âˆ‚y/âˆ‚a = -sin(a) = sin(-a)
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        return out_grad * sin(negate(a))


def cos(a):
    return Cos()(a)



# è‡ªç„¶æŒ‡æ•°å‡½æ•°
class Exp(TensorOp):
    def compute(self, a: NDArray):
        return array_api.exp(a)
    
    '''
    y = exp(a)
    âˆ‚y/âˆ‚a = exp(a)    vÂ¯{a-y} = grad_y * exp(a)  
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        return (out_grad * exp(a),)
    

def exp(a):
    return Exp()(a)



'''
f(x) = log(e_x1 + e_x2 + ... + e_xn)

ä¸ºäº†æ•°å€¼ç¨³å®š,é˜²æ­¢exp(xi)æº¢å‡º,å®é™…é‡‡ç”¨æ•°å€¼ç¨³å®šçš„å½¢å¼
f(x) = max_z + log{e_(x1 - max_Z) + ... + e_(xn - max_Z)}
'''
class LogSumExp(TensorOp):
    def __init__(self, axes: Optional[tuple] = None):
        self.axes = axes

    def compute(self, Z: NDArray):
        # ç”¨äºå¹¿æ’­,å› æ­¤ä¿ç•™ç»´åº¦
        max_Z = array_api.max(Z,axis=self.axes, keepdims=True)
        # ç”¨äºä¸ç»“æœç›¸åŠ ,å› æ­¤èˆå»ç»´åº¦
        max_z = array_api.max(Z,axis=self.axes, keepdims=False)
        
        logsumexp = max_z + array_api.log(
            array_api.sum(array_api.exp(Z - max_Z),axis=self.axes)
        )

        return logsumexp
    
    '''
    TODO: æš‚æ—¶æç½®è¿™éƒ¨åˆ†è®¡ç®—,åç»­å¼„æ‡‚
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        Z = node.inputs[0]
        max_Z = array_api.max(Z.cached_data, axis=self.axes, keepdims=True)
        exp_val = exp(Z - Tensor(max_Z))
        sum_val = summation(exp_val, axes=self.axes)

        log_grad = out_grad / sum_val
        
        input_shape = node.inputs[0].shape
        final_shape = list(input_shape)
        if self.axes:
            if isinstance(self.axes, int):
                final_shape[self.axes] = 1
            else:
                for dim in self.axes:
                    final_shape[dim] = 1
        else:
            final_shape = [1 for _ in range(len(final_shape))]
        sum_grad = reshape(log_grad, tuple(final_shape))
        sum_grad_b = broadcast_to(sum_grad, Z.shape)
        return exp_val * sum_grad_b


def logsumexp(a, axes=None):
    return LogSumExp(axes=axes)(a)



class ReLU(TensorOp):
    def compute(self, a: NDArray):
        return array_api.maximum(a, 0)

    '''
    y = max(a, 0)
    âˆ‚y/âˆ‚a = 1  (a > 0) 
          = 0  (a <= 0)
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        dReLU = array_api.where(a.cached_data > 0, 1.0, 0.0).astype("float32")
        return out_grad * dReLU


def relu(a):
    return ReLU()(a)



class Sigmoid(TensorOp):
    def compute(self, a: NDArray):
        return 1 / (1 + array_api.exp(-a))
    
    '''
    y = sig(a)
    âˆ‚y/âˆ‚a = sig(a)(1 - sig(a))       
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        return out_grad * sigmoid(a) * (1 - sigmoid(a))
    

def sigmoid(a):
    return Sigmoid()(a)



class Tanh(TensorOp):
    def compute(self, a: NDArray):
        return array_api.tanh(a)
    
    '''
    y = tanh(a)
    âˆ‚y/âˆ‚a = 1 - tanh(a)**2
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        a = node.inputs[0]
        return out_grad * (1 + (-tanh(a) ** 2))


def tanh(a):
    return Tanh()(a)



class Flip(TensorOp):
    def __init__(self, axes: Optional[tuple] = None):
        self.axes = axes

    def compute(self, a: NDArray):
        return array_api.flip(a, self.axes)
    
    '''
    è®¸å¤šçº¿æ€§æ“ä½œçš„æ¢¯åº¦ä¼ æ’­å°±æ˜¯è¯¥æ“ä½œçš„é€†æ“ä½œã€‚
    ç›´æ¥å°†æ¢¯åº¦ç¿»è½¬å³å¯
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        return flip(out_grad, self.axes)
    

def flip(a, axes):
    return Flip(axes)(a)



def compact(array):
    out_array = array.copy()
    return out_array



#---------------------------å·ç§¯ç›¸å…³ç®—å­--------------------------

class Dilate(TensorOp):
    '''
        åœ¨çŸ©é˜µå…ƒç´ ä¹‹é—´è¡¥å……é›¶,è¯¥æ“ä½œåœ¨åˆ©ç”¨åå·ç§¯è¿›è¡Œåå‘ä¼ æ’­æ—¶å‡ºç°
        Args:
            axes: éœ€è¦è¡¥å……0çš„ç»´åº¦ 
            dilation: å…ƒç´ ä¹‹é—´è¡¥å……0çš„ä¸ªæ•°
    '''
    def __init__(self, axes: tuple, dilation: int):
        self.axes = axes
        self.dilation = dilation


    def compute(self, a: NDArray):
        shape = a.shape
        out_shape = list(shape)
        
        # ğŸ åœ¨Pythoné‡Œ,sliceæ˜¯ä¸€ä¸ªå†…ç½®ç±»,è¡¨ç¤ºåˆ‡ç‰‡æ“ä½œã€‚ 2:10:2 ç­‰ä»·äº slice(2,10,2)
        # åˆå§‹å…ˆé»˜è®¤åˆ‡ç‰‡åˆ‡å®Œæ‰€æœ‰æ•°æ® slices = [slice(0, shape[0], None), ...]
        slices = [slice(0, out_shape[idx]) for idx in range(len(shape))]

        for ax in self.axes:
            if ax >= len(out_shape):
                continue
            # è†¨èƒ€å¯¹åº”ç»´åº¦
            out_shape[ax] = out_shape[ax] * (1 + self.dilation)
            # é‚£ä¹ˆè¯¥ç»´åº¦çš„åˆ‡ç‰‡éœ€è¦å¢åŠ æ­¥é•¿: slice(0, shape[ax]) => slice(0, shape[ax], 1 + dilation) å®ç°è·³æ­¥åˆ‡ç‰‡
            slices[ax] = slice(0, out_shape[ax], 1 + self.dilation)
        
        # è°ƒç”¨array_deviceæ¨¡å—ä¸‹å‡½æ•°ç¡®å®šdevice
        device = get_device_by_data(a)

        # é¢„å…ˆæ„å»ºè†¨èƒ€åçš„å…¨0çŸ©é˜µ
        out_tensor = array_api.zeros(out_shape, dtype="float32", device=device)

        # ä½¿ç”¨åˆ‡ç‰‡ç´¢å¼•å¤åˆ¶æ•°ç»„aåˆ°æ­£ç¡®ä½ç½®
        out_tensor[tuple(slices)] = a
        return out_tensor
    
    '''
    è®¸å¤šçº¿æ€§æ“ä½œçš„æ¢¯åº¦ä¼ æ’­å°±æ˜¯è¯¥æ“ä½œçš„é€†æ“ä½œã€‚
    å¯¹ä¸Šæ¸¸æ¢¯åº¦è¿›è¡Œç›¸åŒè½¬ç½®æ“ä½œå³å®ç°ä¼ æ’­
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        out_grad = undilate(out_grad, self.axes, self.dilation)
        return out_grad


def dilate(a, axes: tuple, dilation: int):
    return Dilate(axes, dilation)(a)
        


class UnDilate(TensorOp):
    
    def __init__(self, axes: tuple, dilation: int):
        self.axes = axes
        self.dilation = dilation

    def compute(self, a: NDArray):

        shape = a.shape
        
        # é»˜è®¤å…ˆè¿›è¡Œå®Œæ•´åˆ‡ç‰‡
        slices = [slice(0, shape[idx]) for idx in range(len(shape))]
    
        for ax in self.axes:
            if ax >= len(shape):
                continue
            
            # åŸæ¥å†™æˆäº†sliceå¯¼è‡´é”™è¯¯,æ³¨æ„å˜é‡çš„æ‹¼å†™!
            slices[ax] = slice(0, shape[ax], 1 + self.dilation)
        
        # åå‘æ“ä½œæ—¶åˆ‡ç‰‡å°±æ˜¯å¯¹åº”çš„å€¼
        # compactç¡®ä¿åˆ‡ç‰‡åæ•°æ®åœ¨å†…å­˜ä¸­è¿ç»­
        return compact(a[tuple(slices)])
    
    '''
    è®¸å¤šçº¿æ€§æ“ä½œçš„æ¢¯åº¦ä¼ æ’­å°±æ˜¯è¯¥æ“ä½œçš„é€†æ“ä½œã€‚
    å¯¹ä¸Šæ¸¸æ¢¯åº¦è¿›è¡Œç›¸åŒè½¬ç½®æ“ä½œå³å®ç°ä¼ æ’­
    '''
    def gradient(self, out_grad: Tensor, node: Tensor):
        out_grad = dilate(out_grad, self.axes, self.dilation)
        return out_grad


def undilate(a, axes: tuple, dilation: int):
    return UnDilate(axes, dilation)(a)


# å·ç§¯ç®—å­
class Conv(TensorOp):
    
    def __init__(self, stride: Optional[int] = 1, padding: Optional[int] = 0):
        self.stride = stride
        self.padding = padding

   
    def compute(self, X: NDArray, W: NDArray):
        '''
            X: ç‰¹å¾å›¾ (N, H , W, C)
            W: å·ç§¯æ ¸ (K, _ , C_in, C_out)
            ä½¿ç”¨ img2col å°† X, W é‡ç»„
            Y = X @ W 
        '''

        N, _H, _W, C_in  = X.shape
        # (å·ç§¯æ ¸å¤§å°, _ , è¾“å…¥é€šé“æ•°, è¾“å‡ºé€šé“æ•°)
        K, _,  I, C_out = W.shape  

        assert C_in == I, "å·ç§¯æ ¸çš„è¾“å…¥é€šé“ä¸ç‰¹å¾å›¾çš„é€šé“æ•°ä¸åŒ¹é…!"

        # ä»…åœ¨H, Wç»´åº¦åšpadding
        pad_width = [
            (0,0),    
            (self.padding, self.padding),
            (self.padding, self.padding),
            (0,0),
        ]
        # å¾—åˆ°paddingåçš„ç‰¹å¾å›¾X
        X_pad = array_api.pad(X, pad_width, mode='constant', constant_values = 0) if self.padding > 0 else X

        '''
            img2col
            æ¶‰åŠå¤šé€šé“,å¤šå·ç§¯æ ¸æ—¶:
            ç‰¹å¾å›¾é€šé“æ¨ªå‘æ‹¼æ¥
            å·ç§¯æ ¸ä¸ªæ•°çºµå‘æ‹¼æ¥
        '''
        # è®¡ç®—å·ç§¯æ ¸å±•å¼€åçš„ç¬¬ä¸€ç»´åº¦,å³å·ç§¯æ ¸å¤§å°(K * K)ä¸è¾“å…¥é€šé“æ•° 
        inner_dim = K * K * C_in
        
        # è·å–paddingçš„æ­¥å¹…, æ­¥å¹…æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œè¡¨ç¤ºåœ¨å†…å­˜ä¸­æ²¿ç€æ¯ä¸ªç»´åº¦å‰è¿›ä¸€ä¸ªå…ƒç´ æ—¶éœ€è¦è·³è¿‡çš„å­—èŠ‚æ•°ã€‚å®ƒæè¿°äº†å¼ é‡åœ¨å†…å­˜ä¸­çš„å¸ƒå±€æ–¹å¼
        Ns, Hs, Ws, Cs = X_pad.strides
        
        # ä½¿ç”¨å…¬å¼è®¡ç®—è¾“å‡ºç‰¹å¾å›¾çš„ç»´åº¦
        H_out = (_H - K + 2 * self.padding) // self.stride + 1
        W_out = (_W - K + 2 * self.padding) // self.stride + 1
        
        '''
            ä½¿ç”¨as_stridedå±•å¼€ç‰¹å¾å›¾
            as_stridedä¸å¤åˆ¶æ•°æ®,è€Œæ˜¯é€šè¿‡æ”¹å˜å¯¹ç°æœ‰å†…å­˜çš„è§£é‡Šæ–¹å¼æ¥åˆ›å»ºæ–°çš„å¼ é‡è§†å›¾ã€‚
            shape æŒ‡å®šå¯¹è¯¥æ•°æ®çš„è§£é‡Šæ–¹å¼
            strides æŒ‡å®šæ¯æ¬¡åˆ‡æ¢ä¸‹ä¸€ä¸ªæ•°æ®æ—¶çš„æ­¥å¹…(ä¸å·ç§¯æ­¥å¹…åŒºåˆ†!)
        '''
        _X = array_api.as_strided(
            X_pad,
            shape = (N, H_out, W_out, K, K, C_in), # åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„å·ç§¯çª—å£
            strides=(Ns, Hs*self.stride, Ws*self.stride, Hs, Ws, Cs) # å·ç§¯æ ¸ç§»åŠ¨æœ‰æ­¥é•¿,æ‰€ä»¥å…¶ä¸­ä¸¤ç»´æ˜¯éœ€è¦é¢å¤–è®¡ç®—æ­¥é•¿çš„
        ) 

    
        _X_ = compact(_X).reshape((-1, inner_dim))
        
        _W_ = compact(W).reshape((-1, C_out))

        out = _X_ @ _W_

        return out.reshape((N, H_out, W_out, C_out))


    def gradient(self, out_grad: Tensor, node: Tensor):
        '''
            å·ç§¯åå‘ä¼ æ’­:åå·ç§¯å…¬å¼      
        '''
        X, W = node.inputs


        '''
            å¯¹ X çš„å¯¼æ•° (ç”¨äºé“¾æ¥æ¢¯åº¦çš„åå‘ä¼ æ’­)
            Î´x = pad(dilate(Î´z)) * rotate180(W)
        '''
        # paddingéƒ¨åˆ†å·²å†…ç½®åœ¨convä¸­,æ‰€ä»¥å…ˆdilate
        
        # grad: (N, H, W, C)
        grad_dilate = dilate(out_grad, (1,2), self.stride - 1)

        # W: (K, K, Cin, Cout)
        W_r180 = flip(W, (0,1))

        W_r180_T = transpose(W_r180)

        K = W_r180.shape(0)
        
        # åå·ç§¯
        grad_X = conv(grad_dilate, W_r180_T, 1, K - 1 - self.padding)


        '''
            å¯¹ W çš„å¯¼æ•° (ç”¨äºæ›´æ–°å‚æ•°)
            Î´w = X * dilate(Î´z)
        '''
        # grad: (N, H, W, C) -> (W, H, N, C) -> (H, W, N, C)
        grad_dilate = grad_dilate.transpose((0, 2)).transpose((0, 1))
        
        # X : (N, H, W, C) -> (W, H, N, C)
        X_t = transpose(X, (0, 3))

        # åå·ç§¯
        grad_W = conv(X_t, grad_dilate, 1, self.padding)

        grad_W = grad_W.transpose((0, 2)).transpose((0, 1))

        return Tensor(grad_X), grad_W



def conv(a, b, stride=1, padding=1):
    return Conv(stride,padding)(a, b)



